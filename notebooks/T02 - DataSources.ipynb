{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic 2: Collecting Social Media Data\n",
    "\n",
    "This notebook contains examples for using web-based APIs (Application Programmer Interfaces) to download data from social media platforms.\n",
    "Our examples will include:\n",
    "\n",
    "- Reddit\n",
    "- Facebook\n",
    "- Twitter\n",
    "\n",
    "For most services, we need to register with the platform in order to use their API.\n",
    "Instructions for the registration processes are outlined in each specific section below.\n",
    "\n",
    "We will use APIs because they *can* be much faster than manually copying and pasting data from the web site, APIs provide uniform methods for accessing resources (searching for keywords, places, or dates), and it should conform to the platform's terms of service (important for partnering and publications).\n",
    "Note however that each of these platforms has strict limits on access times: e.g., requests per hour, search history depth, maximum number of items returned per request, and similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<img src=\"files/RedditLogo.jpg\" width=\"20%\">\n",
    "\n",
    "## Topic 2.1: Reddit API\n",
    "\n",
    "Reddit's API used to be the easiest to use since it did not require credentials to access data on its subreddit pages.\n",
    "Unfortunately, this process has been changed, and developers now need to create a Reddit application on Reddit's app page located here: (https://www.reddit.com/prefs/apps/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For our first piece of code, we need to import the package \n",
    "# that connects to Reddit. Praw is a thin wrapper around reddit's \n",
    "# web APIs and works well\n",
    "\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Reddit Application\n",
    "Go to https://www.reddit.com/prefs/apps/.\n",
    "Scroll down to \"create application\", select \"web app\", and provide a name, description, and URL (which can be anything).\n",
    "\n",
    "After you press \"create app\", you will be redirected to a new page with information about your application. Copy the unique identifiers below \"web app\" and beside \"secret\". These are your client_id and client_secret values, which you need below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/reddit_screens/0-001.png\" scale=\"10%\"/>\n",
    "<img src=\"files/reddit_screens/1-002.png\" scale=\"20%\"/>\n",
    "<img src=\"files/reddit_screens/1-003.png\" scale=\"10%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we specify a \"unique\" user agent for our code\n",
    "# This is primarily for identification, I think, and some\n",
    "# user-agents of bad actors might be blocked\n",
    "redditApi = praw.Reddit(client_id='OdpBKZ1utVJw8Q',\n",
    "                        client_secret='KH5zzauulUBG45W-XYeAS5a2EdA',\n",
    "                        user_agent='crisis_informatics_v01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Reddit Posts\n",
    "\n",
    "Now for a given subreddit, we can get the newest posts to that sub. \n",
    "Post titles are generally short, so you could treat them as something similar to a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddit = \"worldnews\"\n",
    "\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "\n",
    "submissions = targetSub.new(limit=10)\n",
    "for post in submissions:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging Reddit's Voting\n",
    "\n",
    "Getting the new posts gives us the most up-to-date information. \n",
    "You can also get the \"hot\" posts, \"top\" posts, etc. that should be of higher quality. \n",
    "In theory.\n",
    "__Caveat emptor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddit = \"worldnews\"\n",
    "\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "\n",
    "submissions = targetSub.hot(limit=5)\n",
    "for post in submissions:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following Multiple Subreddits\n",
    "\n",
    "Reddit has a mechanism called \"multireddits\" that essentially allow you to view multiple reddits together as though they were one.\n",
    "To do this, you need to concatenate your subreddits of interesting using the \"+\" sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddit = \"worldnews+aww\"\n",
    "\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "submissions = targetSub.new(limit=10)\n",
    "for post in submissions:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Reddit Comments\n",
    "\n",
    "While you're never supposed to read the comments, for certain live streams or new and rising posts, the comments may provide useful insight into events on the ground or people's sentiment.\n",
    "New posts may not have comments yet though.\n",
    "\n",
    "Comments are attached to the post title, so for a given submission, you can pull its comments directly.\n",
    "\n",
    "Note Reddit returns pages of comments to prevent server overload, so you will not get all comments at once and will have to write code for getting more comments than the top ones returned at first.\n",
    "This pagination is performed using the MoreXYZ objects (e.g., MoreComments or MorePosts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subreddit = \"worldnews\"\n",
    "\n",
    "breadthCommentCount = 5\n",
    "\n",
    "targetSub = redditApi.subreddit(subreddit)\n",
    "submissions = targetSub.hot(limit=1)\n",
    "for post in submissions:\n",
    "    print (post.title)\n",
    "    \n",
    "    post.comment_limit = breadthCommentCount\n",
    "    \n",
    "    # Get the top few comments\n",
    "    for comment in post.comments.list():\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            continue\n",
    "        \n",
    "        print (\"---\", comment.name, \"---\")\n",
    "        print (\"\\t\", comment.body)\n",
    "        \n",
    "        for reply in comment.replies.list():\n",
    "            if isinstance(reply, praw.models.MoreComments):\n",
    "                continue\n",
    "            \n",
    "            print (\"\\t\", \"---\", reply.name, \"---\")\n",
    "            print (\"\\t\\t\", reply.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Functionality\n",
    "\n",
    "Reddit has a deep comment structure, and the code above only goes two levels down (top comment and top comment reply). \n",
    "You can view Praw's additional functionality, replete with examples on its website here: http://praw.readthedocs.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<img src=\"files/FacebookLogo.jpg\" width=\"20%\">\n",
    "\n",
    "## Topic 2.2: Facebook API\n",
    "\n",
    "Getting access to Facebook's API is slightly easier than Twitter's in that you can go to the Graph API explorer, grab an access token, and immediately start playing around with the API.\n",
    "The access token isn't good forever though, so if you plan on doing long-term analysis or data capture, you'll need to go the full OAuth route and generate tokens using the approved paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As before, the first thing we do is import the Facebook\n",
    "# wrapper\n",
    "\n",
    "import facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Facebook Graph\n",
    "\n",
    "Facebook has a \"Graph API\" that lets you explore its social graph. \n",
    "For privacy concerns, however, Facebook's Graph API is extremely limited in the kinds of data it can view.\n",
    "For instance, Graph API applications can now only view profiles of people who already have installed that particular application.\n",
    "These restrictions make it quite difficult to see a lot of Facebook's data.\n",
    "\n",
    "That being said, Facebook does have many popular public pages (e.g., BBC World News), and articles or messages posted by these public pages are accessible.\n",
    "In addition, many posts and comments made in reply to these public posts are also publically available for us to explore.\n",
    "\n",
    "To connect to Facebook's API though, we need an access token (unlike Reddit's API).\n",
    "Fortunately, for research and testing purposes, getting an access token is very easy.\n",
    "\n",
    "#### Acquiring a Facebook Access Token\n",
    "\n",
    "1. Log in to your Facebook account\n",
    "1. Go to Facebook's Graph Explorer (https://developers.facebook.com/tools/explorer/)\n",
    "1. Copy the *long* string out of \"Access Token\" box and paste it in the code cell bedlow\n",
    "\n",
    "<img src=\"files/FacebookInstructions_f1.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fbAccessToken = \"EAACEdEose0cBAKZAZBoGzF6ZAJBk3uSB0gXSgxPrZBJ5nsZCXkM25xZBT0GzVABvsZBOvARxRukoLxhVEyO42QO1D1IInuE1ZBgQfffxh10BC0iHJmnKfNGHn9bY6ioZA8gHTYAXoOGL0A07hZBKXxMKO1yS3ZAPDB50MVGLBxDjJJDWAYBFhUIoeaAaMAZAzxcT4lMZD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Facebook Graph API with this temporary access token (it does expire after maybe 15 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to the graph API, note we use version 2.5\n",
    "graph = facebook.GraphAPI(access_token=fbAccessToken, version='2.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Posts from a Public Page\n",
    "\n",
    "To get a public page's posts, all you need is the name of the page. \n",
    "Then we can pull the page's feed, and for each post on the page, we can pull its comments and the name of the comment's author.\n",
    "While it's unlikely that we can get more user information than that, author name and sentiment or text analytics can give insight into bursting topics and demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What page to look at?\n",
    "targetPage = \"nytimes\"\n",
    "\n",
    "# Other options for pages:\n",
    "# nytimes, bbc, bbcamerica, bbcafrica, redcross, disaster\n",
    "\n",
    "maxPosts = 10 # How many posts should we pull?\n",
    "maxComments = 5 # How many comments for each post?\n",
    "\n",
    "post = graph.get_object(id=targetPage + '/feed')\n",
    "\n",
    "# For each post, print its message content and its ID\n",
    "for v in post[\"data\"][:maxPosts]:\n",
    "    print (\"---\")\n",
    "    print (v[\"message\"], v[\"id\"])\n",
    "        \n",
    "    # For each comment on this post, print its number, \n",
    "    # the name of the author, and the message content\n",
    "    print (\"Comments:\")\n",
    "    comments = graph.get_object(id='%s/comments' % v[\"id\"])\n",
    "    for (i, comment) in enumerate(comments[\"data\"][:maxComments]):\n",
    "        print (\"\\t\", i, comment[\"from\"][\"name\"], comment[\"message\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<img src=\"files/TwitterLogo.png\" width=\"20%\">\n",
    "\n",
    "## Topic 2.1: Twitter API\n",
    "\n",
    "Twitter's API is probably the most useful and flexible but takes several steps to configure. \n",
    "To get access to the API, you first need to have a Twitter account and have a mobile phone number (or any number that can receive text messages) attached to that account.\n",
    "Then, we'll use Twitter's developer portal to create an \"app\" that will then give us the keys tokens and keys (essentially IDs and passwords) we will need to connect to the API.\n",
    "\n",
    "So, in summary, the general steps are:\n",
    "\n",
    "0. Have a Twitter account,\n",
    "1. Configure your Twitter account with your mobile number,\n",
    "2. Create an app on Twitter's developer site, and\n",
    "3. Generate consumer and access keys and secrets.\n",
    "\n",
    "We will then plug these four strings into the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For our first piece of code, we need to import the package \n",
    "# that connects to Twitter. Tweepy is a popular and fully featured\n",
    "# implementation.\n",
    "\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Twitter Credentials\n",
    "\n",
    "For more in-depth instructions for creating a Twitter account and/or setting up a Twitter account to use the following code, I will provide a walkthrough on configuring and generating this information.\n",
    "\n",
    "First, we assume you already have a Twitter account.\n",
    "If this is not true, either create one real quick or follow along.\n",
    "See the attached figures.\n",
    "\n",
    "- __Step 1. Create a Twitter account__ If you haven't already done this, do this now at Twitter.com.\n",
    "\n",
    "- __Step 2. Setting your mobile number__ Log into Twitter and go to \"Settings.\" From there, click \"Mobile\" and fill in an SMS-enabled phone number. You will be asked to confirm this number once it's set, and you'll need to do so before you can create any apps for the next step.\n",
    "\n",
    "<img src=\"files/TwitterInstructions_f1.png\" scale=\"10%\"/>\n",
    "<img src=\"files/TwitterInstructions_f2.png\" scale=\"10%\"/>\n",
    "\n",
    "- __Step 3. Create an app in Twitter's Dev site__ Go to (apps.twitter.com), and click the \"Create New App\" button. Fill in the \"Name,\" \"Description,\" and \"Website\" fields, leaving the callback one blank (we're not going to use it). Note that the website __must__ be a fully qualified URL, so it should look like: http://test.url.com. Then scroll down and read the developer agreement, checking that agree, and finally click \"Create your Twitter application.\"\n",
    "\n",
    "<img src=\"files/TwitterInstructions_f3.png\" scale=\"10%\"/>\n",
    "<img src=\"files/TwitterInstructions_f4.png\"/>\n",
    "\n",
    "- __Step 4. Generate keys and tokens with this app__ After your application has been created, you will see a summary page like the one below. Click \"Keys and Access Tokens\" to view and manage keys. Scroll down and click \"Create my access token.\" After a moment, your page should refresh, and it should show you four long strings of characters and numbers, a consume key, consumer secret, an access token, and an access secret (note these are __case-sensitive__!). Copy and past these four strings into the quotes in the code cell below.\n",
    "\n",
    "<img src=\"files/TwitterInstructions_f5.png\" scale=\"10%\"/>\n",
    "<img src=\"files/TwitterInstructions_f6.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the strings from your Twitter app webpage to populate these four \n",
    "# variables. Be sure and put the strings BETWEEN the quotation marks\n",
    "# to make it a valid Python string.\n",
    "\n",
    "consumer_key = \"IQ03DPOdXz95N3rTm2iMNE8va\"\n",
    "consumer_secret = \"0qGHOXVSX1D1ffP7BfpIxqFalLfgVIqpecXQy9SrUVCGkJ8hmo\"\n",
    "access_token = \"867193453159096320-6oUq9riQW8UBa6nD3davJ0SUe9MvZrZ\"\n",
    "access_secret = \"5zMwq2DVhxBnvjabM5SU2Imkoei3AE6UtdeOQ0tzR9eNU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Twitter\n",
    "\n",
    "Once we have the authentication details set, we can connect to Twitter using the Tweepy OAuth handler, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we use the configured authentication information to connect\n",
    "# to Twitter's API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "print(\"Connected to Twitter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Connection\n",
    "\n",
    "Now that we are connected to Twitter, let's do a brief check that we can read tweets by pulling the first few tweets from our own timeline (or the account associated with your Twitter app) and printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get tweets from our timeline\n",
    "public_tweets = api.home_timeline()\n",
    "\n",
    "# print the first five authors and tweet texts\n",
    "for tweet in public_tweets[:5]:\n",
    "    print (tweet.author.screen_name, tweet.author.name, \"said:\", tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Twitter for Keywords\n",
    "\n",
    "Now that we're connected, we can search Twitter for specific keywords with relative ease just like you were using Twitter's search box.\n",
    "While this search only goes back 7 days and/or 1,500 tweets (whichever is less), it can be powerful if an event you want to track just started.\n",
    "\n",
    "Note that you might have to deal with paging if you get lots of data. Twitter will only return you one page of up to 100 tweets at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our search string\n",
    "queryString = \"earthquake\"\n",
    "\n",
    "# Perform the search\n",
    "matchingTweets = api.search(queryString)\n",
    "\n",
    "print (\"Searched for:\", queryString)\n",
    "print (\"Number found:\", len(matchingTweets))\n",
    "\n",
    "# For each tweet that matches our query, print the author and text\n",
    "print (\"\\nTweets:\")\n",
    "for tweet in matchingTweets:\n",
    "    print (tweet.author.screen_name, tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Queries\n",
    "\n",
    "Twitter's Search API exposes many capabilities, like filtering for media, links, mentions, geolocations, dates, etc.\n",
    "We can access these capabilities directly with the search function.\n",
    "\n",
    "For a list of operators Twitter supports, go here: https://dev.twitter.com/rest/public/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets find only media or links about earthquakes\n",
    "queryString = \"earthquake (filter:media OR filter:links)\"\n",
    "\n",
    "# Perform the search\n",
    "matchingTweets = api.search(queryString)\n",
    "\n",
    "print (\"Searched for:\", queryString)\n",
    "print (\"Number found:\", len(matchingTweets))\n",
    "\n",
    "# For each tweet that matches our query, print the author and text\n",
    "print (\"\\nTweets:\")\n",
    "for tweet in matchingTweets:\n",
    "    print (tweet.author.screen_name, tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Pages\n",
    "\n",
    "As mentioned, Twitter serves results in pages. \n",
    "To get all results, we can use Tweepy's Cursor implementation, which handles this iteration through pages for us in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets find only media or links about earthquakes\n",
    "queryString = \"earthquake (filter:media OR filter:links)\"\n",
    "\n",
    "# How many tweets should we fetch? Upper limit is 1,500\n",
    "maxToReturn = 100\n",
    "\n",
    "# Perform the search, and for each tweet that matches our query, \n",
    "# print the author and text\n",
    "print (\"\\nTweets:\")\n",
    "for status in tweepy.Cursor(api.search, q=queryString).items(maxToReturn):\n",
    "    print (status.author.screen_name, status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Search Functionality\n",
    "\n",
    "The Tweepy wrapper and Twitter API is pretty extensive.\n",
    "You can do things like pull the last 3,200 tweets from other users' timelines, find all retweets of your account, get follower lists, search for users matching a query, etc.\n",
    "\n",
    "More information on Tweepy's capabilities are available at its documentation page: (http://tweepy.readthedocs.io/en/v3.5.0/api.html)\n",
    "\n",
    "Other information on the Twitter API is available here: (https://dev.twitter.com/rest/public/search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Streaming\n",
    "\n",
    "Up to this point, all of our work has been retrospective. \n",
    "An event has occurred, and we want to see how Twitter responded over some period of time. \n",
    "\n",
    "To follow an event in real time, Twitter and Tweepy support Twitter streaming.\n",
    "Streaming is a bit complicated, but it essentially lets of track a set of keywords, places, or users.\n",
    "\n",
    "To keep things simple, I will provide a simple class and show methods for printing the first few tweets.\n",
    "Larger solutions exist specifically for handling Twitter streaming.\n",
    "\n",
    "You could take this code though and easily extend it by writing data to a file rather than the console.\n",
    "I've marked where that code could be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we need to create our own listener for the stream\n",
    "# that will stop after a few tweets\n",
    "class LocalStreamListener(tweepy.StreamListener):\n",
    "    \"\"\"A simple stream listener that breaks out after X tweets\"\"\"\n",
    "    \n",
    "    # Max number of tweets\n",
    "    maxTweetCount = 10\n",
    "    \n",
    "    # Set current counter\n",
    "    def __init__(self):\n",
    "        tweepy.StreamListener.__init__(self)\n",
    "        self.currentTweetCount = 0\n",
    "        \n",
    "        # For writing out to a file\n",
    "        self.filePtr = None\n",
    "        \n",
    "    # Create a log file\n",
    "    def set_log_file(self, newFile):\n",
    "        if ( self.filePtr ):\n",
    "            self.filePtr.close()\n",
    "            \n",
    "        self.filePtr = newFile\n",
    "        \n",
    "    # Close log file\n",
    "    def close_log_file(self):\n",
    "        if ( self.filePtr ):\n",
    "            self.filePtr.close()\n",
    "    \n",
    "    # Pass data up to parent then check if we should stop\n",
    "    def on_data(self, data):\n",
    "\n",
    "        print (self.currentTweetCount)\n",
    "        \n",
    "        tweepy.StreamListener.on_data(self, data)\n",
    "            \n",
    "        if ( self.currentTweetCount >= self.maxTweetCount ):\n",
    "            return False\n",
    "\n",
    "    # Increment the number of statuses we've seen\n",
    "    def on_status(self, status):\n",
    "        self.currentTweetCount += 1\n",
    "        \n",
    "        # Could write this status to a file instead of to the console\n",
    "        print (status.text)\n",
    "        \n",
    "        # If we have specified a file, write to it\n",
    "        if ( self.filePtr ):\n",
    "            self.filePtr.write(\"%s\\n\" % status._json)\n",
    "        \n",
    "    # Error handling below here\n",
    "    def on_exception(self, exc):\n",
    "        print (exc)\n",
    "\n",
    "    def on_limit(self, track):\n",
    "        \"\"\"Called when a limitation notice arrives\"\"\"\n",
    "        print (\"Limit\", track)\n",
    "        return\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        \"\"\"Called when a non-200 status code is returned\"\"\"\n",
    "        print (\"Error:\", status_code)\n",
    "        return False\n",
    "\n",
    "    def on_timeout(self):\n",
    "        \"\"\"Called when stream connection times out\"\"\"\n",
    "        print (\"Timeout\")\n",
    "        return\n",
    "\n",
    "    def on_disconnect(self, notice):\n",
    "        \"\"\"Called when twitter sends a disconnect notice\n",
    "        \"\"\"\n",
    "        print (\"Disconnect:\", notice)\n",
    "        return\n",
    "\n",
    "    def on_warning(self, notice):\n",
    "        print (\"Warning:\", notice)\n",
    "        \"\"\"Called when a disconnection warning message arrives\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the stream using the listener above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listener = LocalStreamListener()\n",
    "localStream = tweepy.Stream(api.auth, listener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stream based on keywords\n",
    "localStream.filter(track=['earthquake', 'disaster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listener = LocalStreamListener()\n",
    "localStream = tweepy.Stream(api.auth, listener)\n",
    "\n",
    "# List of screen names to track\n",
    "screenNames = ['bbcbreaking', 'CNews', 'bbc', 'nytimes']\n",
    "\n",
    "# Twitter stream uses user IDs instead of names\n",
    "# so we must convert\n",
    "userIds = []\n",
    "for sn in screenNames:\n",
    "    user = api.get_user(sn)\n",
    "    userIds.append(user.id_str)\n",
    "\n",
    "# Stream based on users\n",
    "localStream.filter(follow=userIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listener = LocalStreamListener()\n",
    "localStream = tweepy.Stream(api.auth, listener)\n",
    "\n",
    "# Specify coordinates for a bounding box around area of interest\n",
    "# In this case, we use San Francisco\n",
    "swCornerLat = 36.8\n",
    "swCornerLon = -122.75\n",
    "neCornerLat = 37.8\n",
    "neCornerLon = -121.75\n",
    "\n",
    "boxArray = [swCornerLon, swCornerLat, neCornerLon, neCornerLat]\n",
    "\n",
    "# Say we want to write these tweets to a file\n",
    "listener.set_log_file(codecs.open(\"tweet_log.json\", \"w\", \"utf8\"))\n",
    "\n",
    "# Stream based on location\n",
    "localStream.filter(locations=boxArray)\n",
    "\n",
    "# Close the log file\n",
    "listener.close_log_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
